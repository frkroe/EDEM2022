{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["T3T4BpjXqAGx","CLmOllcMZPWN"],"authorship_tag":"ABX9TyNgAnth6+tN4MiruGXZmBzT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"UB-UyzRyNBTa"},"source":["# Redes Neuronales Recurrentes\n","\n","A lo largo de este notebook de prácticas haremos varios ejemplos, tanto con RNNs básicas como con LSTMs, y veremos las diferencias.\n","\n","También trabajaremos con problemas con una única variable y con problemas multivariantes."]},{"cell_type":"markdown","metadata":{"id":"85ruHxMMNWza"},"source":["# 1. Predicción de demanda de vuelos en una aerolinea\n","\n","El primer problema que abordaremos es el de tratar de predecir los viajeros de una determinada aerolinea. Para ello, carguaremos los datos y los transformaremos de forma que podamos utilizarlos con nuestros modelos."]},{"cell_type":"code","metadata":{"id":"zfpw7wlAZ7JS"},"source":["# Nos descargamos los datos\n","!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BU2ANamgZ92g"},"source":["# Y los visualizamos\n","import pandas\n","import matplotlib.pyplot as plt\n","dataset_raw = pandas.read_csv('airline-passengers.csv', usecols=[1], engine='python')\n","plt.plot(dataset_raw)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inspeccionamos los datos:"],"metadata":{"id":"MEci4BtKEoQs"}},{"cell_type":"code","source":["dataset_raw"],"metadata":{"id":"zkX_9rajy_UW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hUok1ze6rVo0"},"source":["### **Transformación de los datos**"]},{"cell_type":"markdown","metadata":{"id":"WFSX7bxX52Zz"},"source":["Ahora necesitamos convertir los datos de forma que la RNN pueda usarlos. Daos cuenta de que por el momento tenemos únicamente un conjunto de valores secuenciales, pero vamos a querer aplicar un algoritmo supervisado (las RNNs lo son).\n","\n","Así que... ¿Dónde están las etiquetas?\n","\n","¡En los propios datos!\n","\n","Nosotros tenemos esto:\n","\n","<img src=\"https://miro.medium.com/max/1400/1*nhffHtudbkqG0yYIF3oeBA.png\">\n","\n","Donde la etiqueta para una determinada secuencia de longitud N será la muestra N+1:\n","\n","<img src=\"https://miro.medium.com/max/1400/1*murkc0tNsvgdQDVgKqSdfQ.png\">"]},{"cell_type":"markdown","metadata":{"id":"h3tuK7EUJI9H"},"source":["Y esta transformación de datos es muy sencilla con Python:"]},{"cell_type":"code","metadata":{"id":"JvgBL7vyN2Fa"},"source":["# convertimos los datos a un formato entendible por la RNN\n","import numpy as np\n","\n","def create_dataset(dataset, look_back_memory=1):\n","    dataX, dataY = [], []\n","    for i in range(len(dataset)-look_back_memory-1):\n","        dataX.append(dataset[i:i+look_back_memory, 0])\n","        dataY.append(dataset[i+look_back_memory, 0])\n","    return np.array(dataX), np.array(dataY)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ni0ihwHYO1TW"},"source":["Imaginémonos que queremos, a partir de un valor, predecir el siguiente. Es decir, tendríamos un \"ventana\" de un único elemento. Para ello, tendríamos que hacer lo siguiente:"]},{"cell_type":"code","metadata":{"id":"RG_UE9yTOv7R"},"source":["dataX, dataY = create_dataset(dataset_raw.values, look_back_memory=1)\n","print(dataX.shape)\n","print(dataY.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CJIyh0ahPHGb"},"source":["¡Atención! Esto no significa que nuestra memoria sea de un elemento. Recordad que las RNNs tienen una \"cinta transportadora\" (estado) en la que acumulan información. Por lo tanto, lo único que indica esta ventana, es la cantidad de muestras en la que se basará para hacer la predicción en cada instante temporal.\n","\n","Pero luego, la RNN, podrá almacenar información de varias de esas predicciones."]},{"cell_type":"markdown","metadata":{"id":"CfSv17grQz22"},"source":["Si quisieramos que nuestra RNN utilizara más de una muestra para cada predicción, tendríamos que aumentar el tamaño de la ventana de 1 a N. Esto podríamos hacerlo modificando el valor de `look_back_memory`."]},{"cell_type":"code","metadata":{"id":"zIeHYm_5Q_RU"},"source":["# Por ejemplo, para una ventana de 10 elementos:\n","dataX, dataY = create_dataset(dataset_raw.values, look_back_memory=10)\n","print(dataX.shape)\n","print(dataY.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eFetOdf6Th10"},"source":["###**Predicción con una RNN básica**"]},{"cell_type":"markdown","metadata":{"id":"OF7tcd5lrZYv"},"source":["Ahora que ya sabemos como transformar los datos al formato pertinente, podemos comenzar a implementar el modelo. \n","\n","En este caso, utilizaremos:\n","\n","- Una red recurrente (`SimpleRNN`) con una celda y `10` neuronas, seguida de una capa `Dense` con activación `linear` (recordad que estamos haciendo regresión).\n","- Como optimizador utilizaremos `Adam` y como función de pérdidas el `mean_squared_error`."]},{"cell_type":"code","metadata":{"id":"_oTS7vDus0Uu"},"source":["import numpy\n","\n","def create_dataset(dataset, look_back_memory=1):\n","    dataX, dataY = [], []\n","    for i in range(len(dataset)-look_back_memory-1):\n","        dataX.append(dataset[i:i+look_back_memory, 0])\n","        dataY.append(dataset[i+look_back_memory, 0])\n","    return numpy.array(dataX), numpy.array(dataY)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ol_zv-xcPwz8"},"source":["# hacemos el import de todo lo que utilizaremos\n","import numpy\n","import matplotlib.pyplot as plt\n","from pandas import read_csv\n","import math\n","from keras.models import Sequential\n","from keras.layers import Dense, SimpleRNN\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qIe8l5dyPw0B"},"source":["# fijamos la semilla para obtener resultados reproducibles\n","numpy.random.seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LYWTLa8IPw0D"},"source":["# cargamos los datos\n","dataframe = read_csv('airline-passengers.csv', usecols=[1], engine='python')\n","dataset = dataframe.values\n","dataset = dataset.astype('float32')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C6yK9XKiPw0D"},"source":["# normalizamos el dataset\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","dataset = scaler.fit_transform(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jGl7PvtuPw0D"},"source":["# dividimos en train y test\n","train_size = int(len(dataset) * 0.67)\n","test_size = len(dataset) - train_size\n","train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q-IZzy5MPw0D"},"source":["# transformamos los datos para crearnos N registros con T timestamps cada uno \n","# (uno por cada instante temporal hasta completar el tamaño de la ventana) y \n","# las V variables de las que disponga nuestro dataset. En este caso, vamos a \n","# escoger una ventana con un único timestamp T=1 y solo tendremos una variable,\n","# con lo que V=1 (número de pasajeros).\n","look_back_memory = 1\n","trainX, trainY = create_dataset(train, look_back_memory)\n","testX, testY = create_dataset(test, look_back_memory)\n","print(trainX.shape, trainY.shape)\n","print(testX.shape, testY.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PlpNZfUQPw0D"},"source":["# Nos aseguramos de que las dimensiones de las entradas son las correctas:\n","# (número de ventanas de T elementos, los T elementos de cada ventana, las V variables de cada timestamp)\n","variables = 1 # (trainX.shape[1])\n","trainX = numpy.reshape(trainX, (trainX.shape[0], look_back_memory, variables))\n","testX = numpy.reshape(testX, (testX.shape[0], look_back_memory, variables))\n","print(trainX.shape)\n","print(testX.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z0oDiDFIPw0E"},"source":["# creamos el modelo y lo entrenamos\n","model = Sequential() #initialize model\n","model.add(SimpleRNN(10, input_shape=(look_back_memory, variables)))\n","model.add(Dense(1, activation='linear'))\n","model.compile(loss='mean_squared_error', optimizer='adam')\n","model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FnQGGyKxPw0E"},"source":["# vamos a ver qué tal funciona nuestro modelo\n","trainPredict = model.predict(trainX)\n","testPredict = model.predict(testX)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QIdbx_aNTICW"},"source":["trainPredict.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M6XsV4wQSynh"},"source":["trainY.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Te3GNJgwPw0E"},"source":["# una vez hechas las predicciones, tenemos que des-normalizarlas\n","trainPredict = scaler.inverse_transform(trainPredict)\n","trainY_orig = scaler.inverse_transform([trainY])\n","testPredict = scaler.inverse_transform(testPredict)\n","testY_orig = scaler.inverse_transform([testY])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lB2fCP0yPw0E"},"source":["# y ahora calculamos el error cometido en train y en test\n","trainScore = math.sqrt(mean_squared_error(trainY_orig[0], trainPredict[:,0]))\n","print('Train Score: %.2f RMSE' % (trainScore))\n","testScore = math.sqrt(mean_squared_error(testY_orig[0], testPredict[:,0]))\n","print('Test Score: %.2f RMSE' % (testScore))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kOXlpMbjPw0F"},"source":["# por como creamos el dataset de entrenamiento, ahora tenemos que desplazar\n","# nuestras predicciones para que \"cuadren\" con el eje x de los datos originales\n","trainPredictPlot = numpy.empty_like(dataset)\n","trainPredictPlot[:, :] = numpy.nan\n","trainPredictPlot[look_back_memory:len(trainPredict)+look_back_memory, :] = trainPredict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pYKX6C-OPw0F"},"source":["# y lo mismo para el test\n","testPredictPlot = numpy.empty_like(dataset)\n","testPredictPlot[:, :] = numpy.nan\n","testPredictPlot[len(trainPredict)+(look_back_memory*2)+1:len(dataset)-1, :] = testPredict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eoHkr4Y5Pw0F"},"source":["# y mostramos los datos originales, la predicción en training y la predicción en test\n","plt.plot(scaler.inverse_transform(dataset))\n","plt.plot(trainPredictPlot)\n","plt.plot(testPredictPlot)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G6GKw8JwZxWd"},"source":["Estos resultados son los que hemos conseguido con una RNN básica. ¿Os animáis a probar con una LSTM?"]},{"cell_type":"markdown","metadata":{"id":"T3T4BpjXqAGx"},"source":["###**Ejercicio 1**\n","\n","Acabamos de entrenar un modelo RNN para predecir la demanda de vuelos por día. Ahora os toca a vosotros: en vez de utilizar la celda RNN, utilizad la LSTM con exactamente la misma configuración."]},{"cell_type":"code","source":["from keras.layers import LSTM\n","\n","# creamos el modelo y lo entrenamos\n","model = Sequential() #initialize model\n","## Aquí vuestro código ## \n","# model.add(...)\n","# ...\n","# model.compile(...)\n","# model.fit(...)"],"metadata":{"id":"ZdYqN8aC21ES"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLY6dolvYdid"},"source":["# vamos a ver qué tal funciona nuestro modelo\n","\n","# completad las lineas siguientes\n","trainPredict = model.predict( ... )\n","testPredict = model.predict( ... )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9PcGXFWYdif"},"source":["# una vez hechas las predicciones, tenemos que des-normalizarlas\n","trainPredict = scaler.inverse_transform(trainPredict)\n","trainY_orig = scaler.inverse_transform([trainY])\n","testPredict = scaler.inverse_transform(testPredict)\n","testY_orig = scaler.inverse_transform([testY])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LqUC8Hw_Ydig"},"source":["# y ahora calculamos el error cometido en train y en test\n","trainScore = math.sqrt(mean_squared_error(trainY_orig[0], trainPredict[:,0]))\n","print('Train Score: %.2f RMSE' % (trainScore))\n","testScore = math.sqrt(mean_squared_error(testY_orig[0], testPredict[:,0]))\n","print('Test Score: %.2f RMSE' % (testScore))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ambz8RCYdih"},"source":["# por como creamos el dataset de entrenamiento, ahora tenemos que desplazar\n","# nuestras predicciones para que \"cuadren\" con el eje x de los datos originales\n","trainPredictPlot = numpy.empty_like(dataset)\n","trainPredictPlot[:, :] = numpy.nan\n","trainPredictPlot[look_back_memory:len(trainPredict)+look_back_memory, :] = trainPredict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vnimxFXiYdih"},"source":["# y lo mismo para el test\n","testPredictPlot = numpy.empty_like(dataset)\n","testPredictPlot[:, :] = numpy.nan\n","testPredictPlot[len(trainPredict)+(look_back_memory*2)+1:len(dataset)-1, :] = testPredict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lMf2Hgy0Ydii"},"source":["# y mostramos los datos originales, la predicción en training y la predicción en test\n","plt.plot(scaler.inverse_transform(dataset))\n","plt.plot(trainPredictPlot)\n","plt.plot(testPredictPlot)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CLmOllcMZPWN"},"source":["###**Ejercicio 2** \n","\n","Entrena un modelo RNN básico igual al primero, pero con una ventana de 10 timestamps."]},{"cell_type":"code","source":["# hacemos el import de todo lo que utilizaremos\n","import numpy\n","import matplotlib.pyplot as plt\n","from pandas import read_csv\n","import math\n","from keras.models import Sequential\n","from keras.layers import Dense, SimpleRNN\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","\n","# fijamos la semilla para obtener resultados reproducibles\n","numpy.random.seed(42)\n","\n","# cargamos los datos\n","dataframe = read_csv('airline-passengers.csv', usecols=[1], engine='python')\n","dataset = dataframe.values\n","dataset = dataset.astype('float32')\n","\n","# normalizamos el dataset\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","dataset = scaler.fit_transform(dataset)\n","\n","# dividimos en train y test\n","train_size = int(len(dataset) * 0.67)\n","test_size = len(dataset) - train_size\n","train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]"],"metadata":{"id":"lPoNWzsd6JAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transformamos los datos para crearnos N registros con T timestamps cada uno \n","# (uno por cada instante temporal hasta completar el tamaño de la ventana) y \n","# las V variables de las que disponga nuestro dataset. En este caso, vamos a \n","# escoger una ventana con un único timestamp T=10 y solo tendremos una variable,\n","# con lo que V=1 (número de pasajeros).\n","\n","\n","# completad las líneas siguientes\n","\n","# look_back_memory = ...\n","# trainX, trainY = create_dataset( ... )\n","# testX, testY = create_dataset( ... )\n","\n","print(trainX.shape, trainY.shape)\n","print(testX.shape, testY.shape)"],"metadata":{"id":"aZD_n4poFUxl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Nos aseguramos de que las dimensiones de las entradas son las correctas:\n","# (número de ventanas de T elementos, los T elementos de cada ventana, las V variables de cada timestamp)\n","variables = 1 # (trainX.shape[1])\n","trainX = numpy.reshape(trainX, (trainX.shape[0], look_back_memory, variables))\n","testX = numpy.reshape(testX, (testX.shape[0], look_back_memory, variables))\n","print(trainX.shape)\n","print(testX.shape)"],"metadata":{"id":"Cdgi5jNwFXEb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creamos el modelo y lo entrenamos\n","model = Sequential() #initialize model\n","\n","# completad las lineas siguientes\n","\n","# model.add( ... )\n","# model.add( ... )\n","# model.compile( ... )\n","# model.fit( ... )"],"metadata":{"id":"UXPXZSb6FbRg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# vamos a ver qué tal funciona nuestro modelo\n","trainPredict = model.predict(trainX)\n","testPredict = model.predict(testX)\n","\n","# una vez hechas las predicciones, tenemos que des-normalizarlas\n","trainPredict = scaler.inverse_transform(trainPredict)\n","trainY_orig = scaler.inverse_transform([trainY])\n","testPredict = scaler.inverse_transform(testPredict)\n","testY_orig = scaler.inverse_transform([testY])\n","\n","# y ahora calculamos el error cometido en train y en test\n","trainScore = math.sqrt(mean_squared_error(trainY_orig[0], trainPredict[:,0]))\n","print('Train Score: %.2f RMSE' % (trainScore))\n","testScore = math.sqrt(mean_squared_error(testY_orig[0], testPredict[:,0]))\n","print('Test Score: %.2f RMSE' % (testScore))\n","\n","# por como creamos el dataset de entrenamiento, ahora tenemos que desplazar\n","# nuestras predicciones para que \"cuadren\" con el eje x de los datos originales\n","trainPredictPlot = numpy.empty_like(dataset)\n","trainPredictPlot[:, :] = numpy.nan\n","trainPredictPlot[look_back_memory:len(trainPredict)+look_back_memory, :] = trainPredict\n","\n","# y lo mismo para el test\n","testPredictPlot = numpy.empty_like(dataset)\n","testPredictPlot[:, :] = numpy.nan\n","testPredictPlot[len(trainPredict)+(look_back_memory*2)+1:len(dataset)-1, :] = testPredict\n","\n","# y mostramos los datos originales, la predicción en training y la predicción en test\n","plt.plot(scaler.inverse_transform(dataset))\n","plt.plot(trainPredictPlot)\n","plt.plot(testPredictPlot)\n","plt.show()"],"metadata":{"id":"VrXbxi6XFdVm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ck4gmTyp9aJ"},"source":["### **Ejercicio 3**\n","\n","Hemos visto que con la RNN y una ventana de 10 timestamps podemos mejorar los resultados obtenidos con una ventana de 1 timestamp. ¿Se os ocurre como intentar mejorar estos resultados?\n","\n","Sabemos que las RNN básicas sufren problemas de memoria, y que las LSTM son capaces de mitigarlos. Además, hemos comprobado de forma práctica que las LSTMs consiguen un mejor ajuste.\n","\n","Entrena una LSTM (https://keras.io/api/layers/recurrent_layers/lstm/) con los datos anteriores, una ventana de 10 timestamps y sin rellenar los T timestamps iniciales con 0's."]},{"cell_type":"code","source":[],"metadata":{"id":"hLtCXzkmF4Hu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transformamos los datos para crearnos N registros con T timestamps cada uno \n","# (uno por cada instante temporal hasta completar el tamaño de la ventana) y \n","# las V variables de las que disponga nuestro dataset. En este caso, vamos a \n","# escoger una ventana con un único timestamp T=10 y solo tendremos una variable,\n","# con lo que V=1 (número de pasajeros).\n","\n","\n","# completad las líneas siguientes\n","\n","# look_back_memory = ...\n","# trainX, trainY = create_dataset( ... )\n","# testX, testY = create_dataset( ... )\n","\n","print(trainX.shape, trainY.shape)\n","print(testX.shape, testY.shape)"],"metadata":{"id":"9D-bUM_lGT-v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Nos aseguramos de que las dimensiones de las entradas son las correctas:\n","# (número de ventanas de T elementos, los T elementos de cada ventana, las V variables de cada timestamp)\n","variables = 1 # (trainX.shape[1])\n","trainX = numpy.reshape(trainX, (trainX.shape[0], look_back_memory, variables))\n","testX = numpy.reshape(testX, (testX.shape[0], look_back_memory, variables))\n","print(trainX.shape)\n","print(testX.shape)"],"metadata":{"id":"Nhq9b52wGT-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creamos el modelo y lo entrenamos\n","model = Sequential() #initialize model\n","\n","# completad las lineas siguientes\n","\n","# model.add( ... )\n","# model.add( ... )\n","# model.compile( ... )\n","# model.fit( ... )"],"metadata":{"id":"_V2GJue8F4Rr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# vamos a ver qué tal funciona nuestro modelo\n","trainPredict = model.predict(trainX)\n","testPredict = model.predict(testX)\n","\n","# una vez hechas las predicciones, tenemos que des-normalizarlas\n","trainPredict = scaler.inverse_transform(trainPredict)\n","trainY_orig = scaler.inverse_transform([trainY])\n","testPredict = scaler.inverse_transform(testPredict)\n","testY_orig = scaler.inverse_transform([testY])\n","\n","# y ahora calculamos el error cometido en train y en test\n","trainScore = math.sqrt(mean_squared_error(trainY_orig[0], trainPredict[:,0]))\n","print('Train Score: %.2f RMSE' % (trainScore))\n","testScore = math.sqrt(mean_squared_error(testY_orig[0], testPredict[:,0]))\n","print('Test Score: %.2f RMSE' % (testScore))"],"metadata":{"id":"INDWeUW6F4Rr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# por como creamos el dataset de entrenamiento, ahora tenemos que desplazar\n","# nuestras predicciones para que \"cuadren\" con el eje x de los datos originales\n","trainPredictPlot = numpy.empty_like(dataset)\n","trainPredictPlot[:, :] = numpy.nan\n","trainPredictPlot[look_back_memory:len(trainPredict)+look_back_memory, :] = trainPredict\n","\n","# y lo mismo para el test\n","testPredictPlot = numpy.empty_like(dataset)\n","testPredictPlot[:, :] = numpy.nan\n","testPredictPlot[len(trainPredict)+(look_back_memory*2)+1:len(dataset)-1, :] = testPredict\n","\n","# y mostramos los datos originales, la predicción en training y la predicción en test\n","plt.plot(scaler.inverse_transform(dataset))\n","plt.plot(trainPredictPlot)\n","plt.plot(testPredictPlot)\n","plt.show()"],"metadata":{"id":"CtjlP6f0JSj4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9iWtqKqoJVZi"},"source":["### **Ejercicio 4**\n","\n","Realiza lo mismo que en el ejercicio 3, pero con unidades GRU. Compara los resultados obtenidos."]},{"cell_type":"code","source":["# necesitarás importar algo..."],"metadata":{"id":"UE1JbUA1JVZj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Nos aseguramos de que las dimensiones de las entradas son las correctas:\n","# (número de ventanas de T elementos, los T elementos de cada ventana, las V variables de cada timestamp)\n","variables = 1 # (trainX.shape[1])\n","trainX = numpy.reshape(trainX, (trainX.shape[0], look_back_memory, variables))\n","testX = numpy.reshape(testX, (testX.shape[0], look_back_memory, variables))\n","print(trainX.shape)\n","print(testX.shape)"],"metadata":{"id":"v9ey8HxsJVZk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creamos el modelo y lo entrenamos\n","model = Sequential() #initialize model\n","\n","# completad las lineas siguientes\n","\n","# model.add( ... )\n","# model.add( ... )\n","# model.compile( ... )\n","# model.fit( ... )"],"metadata":{"id":"FLE9GY_5JVZk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# vamos a ver qué tal funciona nuestro modelo\n","trainPredict = model.predict(trainX)\n","testPredict = model.predict(testX)\n","\n","# una vez hechas las predicciones, tenemos que des-normalizarlas\n","trainPredict = scaler.inverse_transform(trainPredict)\n","trainY_orig = scaler.inverse_transform([trainY])\n","testPredict = scaler.inverse_transform(testPredict)\n","testY_orig = scaler.inverse_transform([testY])\n","\n","# y ahora calculamos el error cometido en train y en test\n","trainScore = math.sqrt(mean_squared_error(trainY_orig[0], trainPredict[:,0]))\n","print('Train Score: %.2f RMSE' % (trainScore))\n","testScore = math.sqrt(mean_squared_error(testY_orig[0], testPredict[:,0]))\n","print('Test Score: %.2f RMSE' % (testScore))"],"metadata":{"id":"g1ABYvt5JVZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# por como creamos el dataset de entrenamiento, ahora tenemos que desplazar\n","# nuestras predicciones para que \"cuadren\" con el eje x de los datos originales\n","trainPredictPlot = numpy.empty_like(dataset)\n","trainPredictPlot[:, :] = numpy.nan\n","trainPredictPlot[look_back_memory:len(trainPredict)+look_back_memory, :] = trainPredict\n","\n","# y lo mismo para el test\n","testPredictPlot = numpy.empty_like(dataset)\n","testPredictPlot[:, :] = numpy.nan\n","testPredictPlot[len(trainPredict)+(look_back_memory*2)+1:len(dataset)-1, :] = testPredict\n","\n","# y mostramos los datos originales, la predicción en training y la predicción en test\n","plt.plot(scaler.inverse_transform(dataset))\n","plt.plot(trainPredictPlot)\n","plt.plot(testPredictPlot)\n","plt.show()"],"metadata":{"id":"fXgXJvQwKEni"},"execution_count":null,"outputs":[]}]}