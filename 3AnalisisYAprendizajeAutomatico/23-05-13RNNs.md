### Clase del 13/05/2023 con Felix
# RNNs: Recurrent Neural Networks

## Problemas temporales
Cómo abordamos problemas con componentes temporales, p.ej.: predicción de compras, predicción de un producto, personas viajando con una determinada aerolínea, etc.

## Definiendo una RNN
Redes Neuronales Recurrentes (RNNs) son redes neuronales que pueden procesar secuencias de entrada y producir una salida. Pueden procesar entradas de longitud variable y son útiles para problemas con componentes temporales.

![RNN](https://editor.analyticsvidhya.com/uploads/86855simplilearn.gif)

![Componentes RNN](https://i.stack.imgur.com/02KvP.png)

### Componentes de una RNN:
- X Entrada: secuencia de entrada
- h Salida: secuencia de salida
- Capa oculta: capa que procesa la entrada y produce una salida
- A: un estado que arrastramos de una entrada a otra (para tener memoria)
- tanh: función de activación 

### Cómo aprenden las RNNs
- Backpropagation Through Time (BPTT): es un algoritmo de backpropagation que se utiliza para entrenar RNNs, es decir que se utiliza para calcular los gradientes de las RNNs a través del tiempo y así actualizar los pesos de las RNNs

## Qué problemas resuelven las RNNs
- Problemas de predicción de series temporales
    - one-to-one: una entrada, una salida (p.ej.: clasificación de imágenes)
    - one-to-many: una entrada, muchas salidas (p.ej.: generación de subtítulos de imágenes)
    - many-to-one: muchas entradas, una salida (p.ej.: clasificación de sentimientos en texto)
    - many-to-many: muchas entradas, muchas salidas (p.ej.: procesamiento de videos)

## Ventajas y desventajas de las RNNs
- Ventajas
    - Pueden procesar componentes temporales
    - Su desempeño no se ve afectado por missing values
    - Permite shuffle, es decir que no es necesario que los datos estén ordenados
- Desventajas
    - No pueden procesar secuencias largas
    - No pueden procesar dependencias a largo plazo porque tienen poco memoria
    - Su entrenamiento es dificilmente paralelizable y muy costoso computacionalmente


## El problema de la memoria
Las RNNs tienen un problema de memoria, es decir que no pueden procesar dependencias a largo plazo. Esto se debe a que las RNNs tienen una memoria muy corta, por lo que no pueden procesar dependencias a largo plazo.

## Long-Short Term Memory (LSTM)
La memoria de las RNNs es muy corta, por lo que no pueden procesar dependencias a largo plazo. Para solucionar esto, se crearon las Long-Short Term Memory (LSTM), que son un tipo de RNNs que tienen memoria a largo plazo.

LSTM tienen una memoria a largo plazo que se puede actualizar, leer y borrar. Esto permite que las LSTM puedan procesar dependencias a largo plazo.

## Gated Recurrent Unit (GRU)
Las Gated Recurrent Unit (GRU) son un tipo de RNNs que tienen una memoria a largo plazo que se puede actualizar, leer y borrar. Esto permite que las GRU puedan procesar dependencias a largo plazo. La puerta de actualización de las GRU es más simple que la de las LSTM.

## Arquitecura encoder-decoder
para lidiar con problemas del tipo many-to-may o sequence-to-sequence, se utiliza la arquitectura encoder-decoder, que consiste en una RNN que procesa la entrada y produce un vector de salida, y luego otra RNN que toma ese vector de salida y produce una secuencia de salida.
- el codificador (encoder) es la primera RNN que procesa la entrada y produce un vector de salida
- el decodificador (decoder) es la segunda RNN que toma ese vector de salida y produce una secuencia de salida

![encoder - decoder arquitectura](https://images.contentstack.io/v3/assets/blt71da4c740e00faaa/blt3f45930f7eaf2556/602ec87c43cdb277fec9baec/1_h3MU2J84XRZ96KmnQXAsMg.gif)