{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop 3: Sentiment anlysis\n",
        "In this workshop we will learn how to train a neural network with text\n",
        "as input to classify IMBD rewievs as positive or negative\n",
        "([info of the data](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)). The main blocks of the workshop are:\n",
        "\n",
        "1. Get the data from Keras repository and visualize it.\n",
        "2. Pre-process the data.\n",
        "3. Design the network.\n",
        "4. Train the network.\n",
        "5. Evaluate the model."
      ],
      "metadata": {
        "id": "pHLGOC9sOrEc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMpZCKmkBeJu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Get the data from Keras repository and visualize it."
      ],
      "metadata": {
        "id": "kwYPpO4DNFfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "# Use the default parameters to keras.datasets.imdb.load_data\n",
        "start_char = 1\n",
        "oov_char = 2\n",
        "index_from = 3\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(start_char=start_char, oov_char=oov_char, index_from=index_from)"
      ],
      "metadata": {
        "id": "Z1TPHApUBhd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "id": "heNEPEYnEZIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the word index file mapping words to indices\n",
        "word_index = imdb.get_word_index()\n",
        "word_index['movie']\n",
        "# Reverse the word index to obtain a dict mapping indices to words\n",
        "# And add `index_from` to indices to sync with `x_train`\n",
        "inverted_word_index = dict(\n",
        "    (i + index_from, word) for (word, i) in word_index.items()\n",
        ")\n",
        "inverted_word_index\n",
        "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
        "inverted_word_index[start_char] = \"[START]\"\n",
        "inverted_word_index[oov_char] = \"[OOV]\""
      ],
      "metadata": {
        "id": "ep06ixX7FyKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inverted_word_index[20]"
      ],
      "metadata": {
        "id": "HXyZJPZLG3mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the first sequence in the dataset\n",
        "decoded_sequence = \" \".join(inverted_word_index[i] for i in X_train[0])\n",
        "decoded_sequence"
      ],
      "metadata": {
        "id": "nGpEhQYsF6QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of different words\n",
        "len(inverted_word_index)"
      ],
      "metadata": {
        "id": "r_m2VuzPII7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training data: \")\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "7q8VpUw6DtQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test data: \")\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "c6dEFaG3EKY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize number of classes\n",
        "print(\"Classes: \")\n",
        "print(np.unique(y_train))\n",
        "print(np.unique(y_test))"
      ],
      "metadata": {
        "id": "TnRereMuEON-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalute the class distribution\n",
        "print('Class distribution')\n",
        "print(np.sum(y_train == 0))\n",
        "print(np.sum(y_train == 1))\n",
        "print(np.sum(y_test == 0))\n",
        "print(np.sum(y_test == 1))"
      ],
      "metadata": {
        "id": "KtYoAVKbRgVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize review length\n",
        "print(\"Review length: \")\n",
        "result = [len(x) for x in X_train]\n",
        "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
        "# plot review length\n",
        "plt.boxplot(result)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gds95agHESH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize review length\n",
        "print(\"Review length: \")\n",
        "result = [len(x) for x in X_test]\n",
        "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
        "# plot review length\n",
        "plt.boxplot(result)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s_9NPDSPJEFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Pre-process the data."
      ],
      "metadata": {
        "id": "AlXYBgJ8NK3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-load the dataset just with the 5000 most common words\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "metadata": {
        "id": "wn0FlzMiK9cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the length of the senteces to a fixed size\n",
        "max_words = 500\n",
        "X_train = pad_sequences(X_train, maxlen=max_words)\n",
        "X_test = pad_sequences(X_test, maxlen=max_words)"
      ],
      "metadata": {
        "id": "x9RPoktqJNCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize review length\n",
        "print(\"Review length: \")\n",
        "result = [len(x) for x in X_train]\n",
        "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
        "# plot review length\n",
        "plt.boxplot(result)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "COuoe-IFKDR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Design the network."
      ],
      "metadata": {
        "id": "g9B7d1PNNnSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, 32, input_length=max_words))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "1c5s9Iu_KFro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Y3mMvHfJN1aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "GdCFg9dBNy56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Train the network."
      ],
      "metadata": {
        "id": "wCg_sI7ZNupg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128, verbose=2)"
      ],
      "metadata": {
        "id": "OjPc3ItsKmP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "# plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xBoUYR4kONUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Evaluate the model."
      ],
      "metadata": {
        "id": "d1suiQ7POWnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "metadata": {
        "id": "Cgt1VU_1MxW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice 1: Use the learning of the lesson to reduce de overfitting"
      ],
      "metadata": {
        "id": "0CupmfegQKwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-load the dataset just with the 5000 most common words\n"
      ],
      "metadata": {
        "id": "TEKv7KnZR6iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the length of the senteces to a fixed size\n"
      ],
      "metadata": {
        "id": "8peiujNFSBFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n"
      ],
      "metadata": {
        "id": "FvJg07a6QJEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a summary of the model\n"
      ],
      "metadata": {
        "id": "XKd0InUORQr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n"
      ],
      "metadata": {
        "id": "Fnxu7MelQ8CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n"
      ],
      "metadata": {
        "id": "cujNjAoXRDOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation accuracy\n"
      ],
      "metadata": {
        "id": "PNeCRc7ORFme"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}