{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"szsImiPr1ftc"},"source":["<font color=\"#CA0032\"><h1 align=\"left\">**Redes Generativas Adversariales Condicionales (C-GANs)**</h1></font>\n","\n","<font color=\"#6E6E6E\"><h1 align=\"left\">**Creación de caracteres manuscritos nuevos entrenando con MNIST:**</h1></font>\n","\n","<font color=\"#6E6E6E\"><h1 align=\"left\">**Modelos Deep Learning**</h1></font>\n","\n","<h2 align=\"left\">Manuel Sánchez-Montañés</h2>\n","\n","<font color=\"#6E6E6E\"><h2 align=\"left\">manuel.smontanes@gmail.com</h2></font>"]},{"cell_type":"code","metadata":{"id":"V9iyii47sapd"},"source":["# Basado en:\n","#\n","# https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mj1x9lTosrFf"},"source":["**Para más información:**\n","\n","**Libros:**\n","\n","- Capítulo 20 (\"Deep Generative Models\") de \"Deep Learning\" (2016): https://amzn.to/2YuwVjL\n","\n","- Capítulo 8 (\"Generative Deep Learning\") de \"Deep Learning with Python\" (2017): https://amzn.to/2U2bHuP\n","\n","**Artículos:**\n","\n","- Generative Adversarial Networks (2014): https://arxiv.org/abs/1406.2661\n","\n","- Tutorial: Generative Adversarial Networks, NIPS (2016): https://arxiv.org/abs/1701.00160\n","\n","- Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (2015): https://arxiv.org/abs/1511.06434\n","\n","- Conditional Generative Adversarial Nets (2014): https://arxiv.org/abs/1411.1784\n","\n","- Image-To-Image Translation With Conditional Adversarial Networks (2017): https://arxiv.org/abs/1611.07004\n","\n","- Conditional Generative Adversarial Nets For Convolutional Face Generation (2015): https://www.foldl.me/uploads/2015/conditional-gans-face-generation/paper.pdf"]},{"cell_type":"code","metadata":{"id":"EPPx0ntx10Bn"},"source":["COLAB    = True\n","GRAPHVIZ = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GVFxiBQEAeU2"},"source":["# example of training an conditional gan on the fashion mnist dataset\n","from numpy import expand_dims, zeros, ones, asarray\n","from numpy.random import randn, randint\n","from keras.datasets.mnist import load_data\n","from keras.optimizers import Adam\n","from keras.models import Model\n","from keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU\n","from keras.layers import Dropout, Embedding, Concatenate\n","\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5DHqUwNQBJSG"},"source":["# load fashion mnist images\n","def load_real_samples():\n","\t# load dataset\n","\t(trainX, trainy), (_, _) = load_data()\n","\t# expand to 3d, e.g. add channels\n","\tX = expand_dims(trainX, axis=-1)\n","\t# convert from ints to floats\n","\tX = X.astype('float32')\n","\t# scale from [0,255] to [-1,1]\n","\tX = (X - 127.5) / 127.5\n","\treturn [X, trainy]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s5gP4QYsAXvt"},"source":["# load image data\n","dataset = load_real_samples()\n","X, y = dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4RH-ZGNKZgb"},"source":["X.shape, y.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mEMuqDa1KMRb"},"source":["n_ejemplos = 15\n","\n","for i in randint(low=0, high=len(X), size=n_ejemplos):\n","    plt.imshow(X[i].reshape(28,28)) # cmap=\"gray\"\n","    plt.title(\"Clase: {}\".format(y[i]))\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LkbtvkBoA6AG"},"source":["# define the standalone generator model\n","def define_generator(latent_dim, n_classes=10):\n","\t# label input\n","\tin_label = Input(shape=(1,))\n","\t# embedding for categorical input\n","\tli = Embedding(n_classes, 50)(in_label)\n","\t# linear multiplication\n","\tn_nodes = 7 * 7\n","\tli = Dense(n_nodes)(li)\n","\t# reshape to additional channel\n","\tli = Reshape((7, 7, 1))(li)\n","\t# image generator input\n","\tin_lat = Input(shape=(latent_dim,))\n","\t# foundation for 7x7 image\n","\tn_nodes = 32 * 7 * 7\n","\tgen = Dense(n_nodes)(in_lat)\n","\tgen = LeakyReLU(alpha=0.2)(gen)\n","\tgen = Reshape((7, 7, 32))(gen)\n","\t# merge image gen and label input\n","\tmerge = Concatenate()([gen, li])\n","\t# upsample to 14x14\n","\tgen = Conv2DTranspose(32, (4,4), strides=(2,2), padding='same')(merge)\n","\tgen = LeakyReLU(alpha=0.2)(gen)\n","\t# upsample to 28x28\n","\tgen = Conv2DTranspose(32, (4,4), strides=(2,2), padding='same')(gen)\n","\tgen = LeakyReLU(alpha=0.2)(gen)\n","\t# output\n","\tout_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n","\t# define model\n","\tmodel = Model([in_lat, in_label], out_layer)\n","\treturn model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aa8nYwQ0A59W"},"source":["# define the standalone discriminator model\n","def define_discriminator(in_shape=(28,28,1), n_classes=10):\n","\t# label input\n","\tin_label = Input(shape=(1,))\n","\t# embedding for categorical input\n","\tli = Embedding(n_classes, 50)(in_label)\n","\t# scale up to image dimensions with linear activation\n","\tn_nodes = in_shape[0] * in_shape[1]\n","\tli = Dense(n_nodes)(li)\n","\t# reshape to additional channel\n","\tli = Reshape((in_shape[0], in_shape[1], 1))(li)\n","\t# image input\n","\tin_image = Input(shape=in_shape)\n","\t# concat label as a channel\n","\tmerge = Concatenate()([in_image, li])\n","\t# downsample\n","\tfe = Conv2D(32, (3,3), strides=(2,2), padding='same')(merge)\n","\tfe = LeakyReLU(alpha=0.2)(fe)\n","\t# downsample\n","\tfe = Conv2D(32, (3,3), strides=(2,2), padding='same')(fe)\n","\tfe = LeakyReLU(alpha=0.2)(fe)\n","\t# flatten feature maps\n","\tfe = Flatten()(fe)\n","\t# dropout\n","\tfe = Dropout(0.4)(fe)\n","\t# output\n","\tout_layer = Dense(1, activation='sigmoid')(fe)\n","\t# define model\n","\tmodel = Model([in_image, in_label], out_layer)\n","\t# compile model\n","\topt = Adam(lr=0.0002, beta_1=0.5)\n","\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\treturn model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-7C7y-tA6DC"},"source":["# define the combined generator and discriminator model, for updating the generator\n","def define_gan(g_model, d_model):\n","\t# make weights in the discriminator not trainable\n","\td_model.trainable = False\n","\t# get noise and label inputs from generator model\n","\tgen_noise, gen_label = g_model.input\n","\t# get image output from the generator model\n","\tgen_output = g_model.output\n","\t# connect image output and label input from generator as inputs to discriminator\n","\tgan_output = d_model([gen_output, gen_label])\n","\t# define gan model as taking noise and label and outputting a classification\n","\tmodel = Model([gen_noise, gen_label], gan_output)\n","\t# compile model\n","\topt = Adam(lr=0.0002, beta_1=0.5)\n","\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n","\treturn model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MC5yAmsIBJVO"},"source":["# select real samples\n","def generate_real_samples(dataset, n_samples):\n","\t# split into images and labels\n","\timages, labels = dataset\n","\t# choose random instances\n","\tix = randint(0, images.shape[0], n_samples)\n","\t# select images and labels\n","\tX, labels = images[ix], labels[ix]\n","\t# generate class labels\n","\ty = ones((n_samples, 1))\n","\treturn [X, labels], y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZGoP5zdBJYe"},"source":["# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n_samples, n_classes=10):\n","\t# generate points in the latent space\n","\tx_input = randn(latent_dim * n_samples)\n","\t# reshape into a batch of inputs for the network\n","\tz_input = x_input.reshape(n_samples, latent_dim)\n","\t# generate labels\n","\tlabels = randint(0, n_classes, n_samples)\n","\treturn [z_input, labels]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mnpcr8KhBUhU"},"source":["# use the generator to generate n fake examples, with class labels\n","def generate_fake_samples(generator, latent_dim, n_samples):\n","\t# generate points in latent space\n","\tz_input, labels_input = generate_latent_points(latent_dim, n_samples)\n","\t# predict outputs\n","\timages = generator.predict([z_input, labels_input])\n","\t# create class labels\n","\ty = zeros((n_samples, 1))\n","\treturn [images, labels_input], y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1F7SajvODrqk"},"source":["# size of the latent space\n","latent_dim = 100\n","\n","# create the generator\n","generator = define_generator(latent_dim)\n","generator.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nJoW-6rgDqbB"},"source":["if GRAPHVIZ:\n","    from IPython.display import SVG,display\n","    from keras.utils.vis_utils import model_to_dot\n","    if COLAB:\n","        display(SVG(model_to_dot(generator,show_shapes=True,dpi=64).create(prog='dot', format='svg')))\n","    else:\n","        display(SVG(model_to_dot(generator,show_shapes=True).create(prog='dot', format='svg')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DR3CBeefFsql"},"source":["# create the discriminator\n","discriminator = define_discriminator()\n","\n","discriminator.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VLZx6orRFxwt"},"source":["if GRAPHVIZ:\n","    from IPython.display import SVG,display\n","    from keras.utils.vis_utils import model_to_dot\n","    if COLAB:\n","        display(SVG(model_to_dot(discriminator,show_shapes=True,dpi=64).create(prog='dot', format='svg')))\n","    else:\n","        display(SVG(model_to_dot(discriminator,show_shapes=True).create(prog='dot', format='svg')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xlo7izfoGN6u"},"source":["# create the gan\n","gan = define_gan(generator, discriminator)\n","\n","gan.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0CoiSBF_GVx8"},"source":["if GRAPHVIZ:\n","    from IPython.display import SVG,display\n","    from keras.utils.vis_utils import model_to_dot\n","    if COLAB:\n","        display(SVG(model_to_dot(gan,show_shapes=True,dpi=64).create(prog='dot', format='svg')))\n","    else:\n","        display(SVG(model_to_dot(gan,show_shapes=True).create(prog='dot', format='svg')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i3ArLR-NIRO3"},"source":["# example of loading the generator model and generating images\n","from keras.models import load_model\n","\n","# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n_samples, n_classes=10):\n","\t# generate points in the latent space\n","\tx_input = randn(latent_dim * n_samples)\n","\t# reshape into a batch of inputs for the network\n","\tz_input = x_input.reshape(n_samples, latent_dim)\n","\t# generate labels\n","\tlabels = randint(0, n_classes, n_samples)\n","\treturn [z_input, labels]\n","\n","# create and save a plot of generated images\n","def show_plot(examples, n=10):\n","\t# plot images\n","\tfor i in range(n * n):\n","\t\t# define subplot\n","\t\tplt.subplot(n, n, 1 + i)\n","\t\t# turn off axis\n","\t\tplt.axis('off')\n","\t\t# plot raw pixel data\n","\t\tplt.imshow(examples[i, :, :, 0], cmap='gray_r')\n","\tplt.show()\n","\n","def plotGeneratedImages(generator, latent_dim, order_class=False):\n","\t\tlatent_points, labels = generate_latent_points(latent_dim, 100)\n","\t\tif order_class:\n","\t\t\t\tlabels = asarray([x for _ in range(10) for x in range(10)])\n","\t\t# generate images\n","\t\tX  = generator.predict([latent_points, labels])\n","\t\t# scale from [-1,1] to [0,1]\n","\t\tX = (X + 1) / 2.0\n","\t\t# plot the result\n","\t\tshow_plot(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n0oxO706AX-R"},"source":["# train the generator and discriminator\n","n_epochs=100\n","n_batch=128\n","\n","bat_per_epo = int(dataset[0].shape[0] / n_batch)\n","half_batch = int(n_batch / 2)\n","# manually enumerate epochs\n","for i in range(n_epochs):\n","    # enumerate batches over the training set\n","    print(\"EPOCH\", i+1)\n","    for j in range(bat_per_epo):\n","        # get randomly selected 'real' samples\n","        [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n","        # update discriminator model weights\n","        d_loss1, _ = discriminator.train_on_batch([X_real, labels_real], y_real)\n","        # generate 'fake' examples\n","        [X_fake, labels], y_fake = generate_fake_samples(generator, latent_dim, half_batch)\n","        # update discriminator model weights\n","        d_loss2, _ = discriminator.train_on_batch([X_fake, labels], y_fake)\n","        # prepare points in latent space as input for the generator\n","        [z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n","        # create inverted labels for the fake samples\n","        y_gan = ones((n_batch, 1))\n","        # update the generator via the discriminator's error\n","        g_loss = gan.train_on_batch([z_input, labels_input], y_gan)\n","        # summarize loss on this batch\n","        if i%5 == 0:\n","            print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n","              (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n","    if i%5 == 0:\n","        plotGeneratedImages(generator, latent_dim)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RaaAS6y8JYUV"},"source":["# save the models\n","\n","generator.save(\"mnist_cgan_generator.h5\")\n","generator.save_weights(\"mnist_cgan_generator_weights.h5\")\n","discriminator.save(\"mnist_cgan_discriminator.h5\")\n","discriminator.save_weights(\"mnist_cgan_discriminator_weights.h5\")\n","gan.save(\"mnist_cgan_gan.h5\")\n","gan.save_weights(\"mnist_cgan_gan_weights.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Opeab2KTZT7C"},"source":["!ls -la"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ozhbCKn1Lkkl"},"source":["### **Generación de imágenes nuevas**"]},{"cell_type":"code","metadata":{"id":"tJU2qyIrIRR1"},"source":["# load model\n","model = load_model('mnist_cgan_generator.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oB4tKLRlIRVD"},"source":["plotGeneratedImages(model, latent_dim, order_class=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MuvdUM4TZeV3"},"source":["if COLAB:\n","    from google.colab import files\n","    files.download('mnist_cgan_generator.h5')\n","    files.download('mnist_cgan_generator_weights.h5')\n","    files.download('mnist_cgan_discriminator.h5')\n","    files.download('mnist_cgan_discriminator_weights.h5')\n","    files.download('mnist_cgan_gan.h5')\n","    files.download('mnist_cgan_gan_weights.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hLQycuxmDepU"},"source":["Ejercicios:\n","\n","- Adaptar el ejercicio a la base de datos FMNIST\n","- Adaptar el ejercicio a la base de datos CIFAR-10"]},{"cell_type":"code","source":["from keras.datasets.fashion_mnist import load_data"],"metadata":{"id":"0sxfpHjYEQMl"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUC7dQI_ZswN"},"source":["from tensorflow.keras.datasets.cifar10 import load_data as load_data_cifar10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BQd-q3roDt-J"},"source":["?load_data_cifar10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ow9xvORwDy7s"},"source":[],"execution_count":null,"outputs":[]}]}