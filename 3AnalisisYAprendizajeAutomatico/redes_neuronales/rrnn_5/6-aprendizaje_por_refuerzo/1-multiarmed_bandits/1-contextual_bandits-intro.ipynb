{"cells":[{"cell_type":"markdown","metadata":{"id":"siyhBQHeFD5O"},"source":["<font color=\"#CA3532\"><h1 align=\"left\">**Aprendizaje por Refuerzo**</h1></font>\n","<font color=\"#6E6E6E\"><h2 align=\"left\">**Multiarmed Bandits Contextuales**</h2></font>"]},{"cell_type":"markdown","metadata":{"id":"cCMsv06YHZPX"},"source":["En este notebook se considera el siguiente problema:\n","\n","Tenemos un conjunto de clientes, y de cada cliente tenemos dos datos:\n","\n","* **Edad**\n","* **Ingresos generados** hasta el momento por ese cliente (gasto de ese cliente en nuestra empresa).\n","\n","Nuestra compañía ofrece tres promociones, y el problema es encontrar qué promoción ofrecemos a cada cliente. Una vez que se ofrece una promoción a un cliente, este puede ignorarla o acogerse a ella, generando en este caso una ganancia para la compañía. Por tanto, las **acciones** posibles son las promociones que le podemos ofrecemos al cliente (tres en este ejemplo).\n","\n","En caso de que un cliente se acoja a una promoción la ganancia para la compañía será:\n","\n","* **Promoción 0**: ganancia=10\n","* **Promoción 1**: ganancia=25\n","* **Promoción 2**: ganancia=100\n","\n","\n","Hasta aquí son datos que la compañía tiene bajo control.\n","Sin embargo la compañía desconoce cómo van a responder los clientes a las promociones. Es decir, desconoce la probabilidad de que un cliente $x_i$ se acoja a la promoción $j$ en caso de que se le ofrezca. Es decir, se desconocen las siguientes cantidades:\n","\n","* $p(acoge\\,promo \\,\\vert\\, x_i, le \\, ofrecemos \\, promo \\,0)$\n","* $p(acoge\\,promo \\,\\vert\\, x_i, le \\, ofrecemos \\, promo \\,1)$\n","* $p(acoge\\,promo \\,\\vert\\, x_i, le \\, ofrecemos \\, promo \\,2)$\n","\n","\n","y la decisión óptima sería, una vez calculadas estas tres cantidades para un cliente, elegir la promoción que conlleve mayor ganancia esperada.\n","\n","Si supiéramos el valor de estas cantidades para cada cliente (se pueden ver como funciones que dependen del cliente y la promo ofrecida), se podría calcular la ganancia esperada de cada acción (promo ofrecida) como:\n","\n","* Si se ofrece promo 0:\n","$$\n","Q\\,teórica(promo \\,0, x_i) = 10 \\cdot \n","p(acoge\\,promo \\,\\vert\\, x_i, le \\, ofrecemos \\, promo \\,0)\n","$$\n","* Si se ofrece promo 1:\n","$$\n","Q\\,teórica(promo \\,1, x_i) = 25 \\cdot \n","p(acoge\\,promo \\,\\vert\\, x_i, le \\, ofrecemos \\, promo \\,1)\n","$$\n","* Si se ofrece promo 2:\n","$$\n","Q\\,teórica(promo \\,2, x_i) = 100 \\cdot \n","p(acoge\\,promo \\,\\vert\\, x_i, le \\, ofrecemos \\, promo \\,2)\n","$$\n","\n","\n","Sin embargo, **es imposible** conocer a priori esas cantidades.\n","La compañía espera que cada cliente responda de una manera diferente a las promociones de acuerdo a sus características, pero la compañía desconoce los segmentos de clientes y cómo responden a las promociones.\n","\n","**El enorme valor del modelo contextual bandit** está en que dicha técnica va a construir esos segmentos y las funciones que modelan la respuesta de cada cliente a cada promo. Lo va a hacer de manera **online** (incremental, según se van ofreciendo promos a clientes) y de tal forma que va a intentar maximizar la ganancia obtenida en todo el proceso (incluyendo los pasos iniciales de recolección de información)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTdSXTWkdwW6"},"outputs":[],"source":["COLAB = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VvPS9XIwjco6"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6rNIP_QdyZI"},"outputs":[],"source":["if COLAB:\n","    from google_drive_downloader import GoogleDriveDownloader as gdd\n","    gdd.download_file_from_google_drive(file_id='1tcbutY0wW-JWodaVSTuAhxn4pqPVyqZX',\n","                                        dest_path='./simula_clientes_problema1.py') # simulamos los clientes\n","    gdd.download_file_from_google_drive(file_id='1fCnGzS5U_x-k_03op_XJkHVS4jpvjSxS',\n","                                        dest_path='./spacebandits.zip', unzip=True) # el algoritmo de aprendizaje por refuerzo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2_SI36ddt7m"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from random import random, randint\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","%config InlineBackend.figure_format='retina'\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-HumQ9ppdt7u"},"outputs":[],"source":["from simula_clientes_problema1 import cliente"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDqpL0WldUU_"},"outputs":[],"source":["c = cliente() # x es una instancia de la clase \"cliente\", que tengo implementada en simula_clientes_problema1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58qe_9URdUeY"},"outputs":[],"source":["# edad, ingresos generados en la compañía hasta ahora\n","c.get_context() # mi simulador me devuelve un array con las características del cliente"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VC8dHTdpH2Nx"},"outputs":[],"source":["accion = 0\n","c.get_reward(accion) # me devuelve el reward que obtengo por la acción que he tomado (si es 0, el cliente no ha comprado nada)"]},{"cell_type":"markdown","metadata":{"id":"yc8Ieh7wrXwY"},"source":["### **Estrategia 1: recomendación al azar**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FF5LfIRkdUqG"},"outputs":[],"source":["N = 10000 # número de clientes que voy a simular\n","reward_total = 0\n","for _ in range(N):\n","  c = cliente() # genero un nuevo cliente\n","  accion = np.random.choice(3) # tomo una acción aleatoria\n","  reward_total += c.get_reward(accion) # obtengo el reward que me da esa acción (si lo compra o no)\n","\n","print(\"Reward promedio:\", reward_total/N) "]},{"cell_type":"markdown","metadata":{"id":"VX9TCp5zriOo"},"source":["### **Estrategia 2: Multiarmed Bandit contextual**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TwX_cOq3dCJb"},"outputs":[],"source":["from space_bandits import LinearBandits\n","\n","n_acciones = 3 # número de promociones diferentes\n","n_features = 2 # número de variables de contexto (edad e ingresos generados)\n","\n","N = 10000\n","\n","agente = LinearBandits(n_acciones, n_features, initial_pulls=100) # initial_pulls: duración de la fase de exploración pura (los primeros 100 clientes reciben una promoción aleatoria por no tener un dataset al principio)\n","reward_promedio = []\n","reward_total = 0\n","for i in tqdm(range(N)):\n","  c = cliente() # cliente con el que contacto\n","  contexto = c.get_context()\n","  accion = agente.action(contexto) # choose_action: elegir la acción que voy a tomar con el cliente\n","  reward = c.get_reward(accion) # reward por haber comprado o no\n","  agente.update(contexto, accion, reward) # actualización del modelo para seguir aprendiendo\n","\n","  reward_total += reward\n","  reward_promedio.append(reward_total/(i+1)) # i+1 es el número de iteraciones\n","\n","print(\"Reward promedio:\", reward_total/N)\n","plt.plot(reward_promedio)\n","plt.title(\"Evolución del reward promedio\");"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Con esa estragia 2 hemos doplado la ganancia esperada de la estrategia 1."]},{"cell_type":"markdown","metadata":{"id":"xvFV6nhvXNhq"},"source":["**¿Cómo evalúa el modelo cada una de las acciones?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LN8noXfBdCMO"},"outputs":[],"source":["c = cliente()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYjCEay5dCQz"},"outputs":[],"source":["contexto = c.get_context()\n","contexto"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gnVsTI4lkyj"},"outputs":[],"source":["agente.action(contexto) # nos devuele el producto recomendado para ese cliente (0, 1 o 2) con el Q-valor más alto"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGfBd-Qqtd_c"},"outputs":[],"source":["agente.expected_values(contexto) # Q(s,a) por cada acción a, y para el cliente s (s=contexto)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3rqQvTgV9LH"},"outputs":[],"source":["# Parámetros que aprende el agente:\n","# 3 arrays (porque hay 3 acciones posibles, 3 productos)\n","# En cada array el último elemento es el término constante de un modelo lineal\n","# Los elementos primero y segundo son los coeficientes de edad e ingresos respectivamente\n","agente.mu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3EeU6bd9V9OG"},"outputs":[],"source":["# Separo los parámetros del contexto en edad e ingresos\n","edad, ingresos_generados = contexto\n","edad, ingresos_generados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n0UyadVYWKAq"},"outputs":[],"source":["agente.mu[0][-1] + agente.mu[0][0]*edad + agente.mu[0][1]*ingresos_generados # Q(s,a) para el producto 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-uKM6ECbTYbY"},"outputs":[],"source":["agente.mu[1][-1] + agente.mu[1][0]*edad + agente.mu[1][1]*ingresos_generados # Q(s,a) para el producto 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blqpXlWKWKDq"},"outputs":[],"source":["agente.mu[2][-1] + agente.mu[2][0]*edad + agente.mu[2][1]*ingresos_generados # Q(s,a) para el producto 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mpZN_J8qWKGc"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":0}
