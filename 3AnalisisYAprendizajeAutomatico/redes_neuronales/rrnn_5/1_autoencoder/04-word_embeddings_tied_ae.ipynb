{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"colab":{"provenance":[],"collapsed_sections":["PwBI6XhHdURm","Dguzu5iTdURn"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"DmPCK5r4dUQs"},"source":["<font color=\"#6E6E6E\"><h2 align=\"left\">Text Analytics - Autoencoder</h2></font>"]},{"cell_type":"markdown","metadata":{"id":"7d2UFCqVdUQz"},"source":[" Analysis of a subset of a complaints dataset using 2 classes."]},{"cell_type":"code","metadata":{"id":"ljzHQU5GP72l"},"source":["COLAB = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aLtXeKUDdUQ0"},"source":["## Import main libraries"]},{"cell_type":"code","metadata":{"id":"WWjvuLmGdUQ1"},"source":["import nltk # !pip install nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","from matplotlib.ticker import MaxNLocator\n","\n","%matplotlib inline\n","\n","lemmatizer = WordNetLemmatizer().lemmatize"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xBEitle8dUQ4"},"source":["## Load dataset (\"corpus\")"]},{"cell_type":"code","metadata":{"id":"GFPCuGLydUQ5"},"source":["# El dataset está en:\n","# https://drive.google.com/file/d/1LFW1GSVkZXyXKFUdKNZA8alKK64d5J9P\n","\n","if COLAB:\n","    from google_drive_downloader import GoogleDriveDownloader as gdd\n","    gdd.download_file_from_google_drive(file_id='1LFW1GSVkZXyXKFUdKNZA8alKK64d5J9P',\n","                                        dest_path='./reporting_complaints_2classes.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsJpR2JQQGcT"},"source":["# Read the input\n","df = pd.read_csv(\"./reporting_complaints_2classes.csv\") # the dataset is loaded into a Pandas DataFrame\n","print(df.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aSdHbdXRdUQ7"},"source":["df.sample(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"meP3ht1udUQ9"},"source":["df['product'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJBTph-UdUQ-"},"source":["narratives = list(df['consumer_complaint_narrative'])\n","labels = np.array(df['product'])\n","print(\"Number of narratives: {}\".format(len(narratives)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zi5HzyfndUQ_"},"source":["case = 5\n","print(\"Type of product:\", labels[case])\n","narratives[case]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ngf4JEZ2dURA"},"source":["## Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"tBlk3gUndURB"},"source":["## Training-test split"]},{"cell_type":"code","metadata":{"id":"CO8epa9edURC"},"source":["Nval = 1000\n","\n","from sklearn.model_selection import train_test_split\n","\n","narratives_trval, narratives_te, y_trval, y_te = train_test_split(narratives, labels,\n","                                                                  test_size=0.3, random_state=1)\n","\n","narratives_tr = narratives_trval[:-Nval]\n","y_tr = y_trval[:-Nval]\n","\n","narratives_val = narratives_trval[-Nval:]\n","y_val = y_trval[-Nval:]\n","\n","print(\"Number of training examples:\", len(y_tr))\n","print(\"Number of validation examples:\", len(y_val))\n","print(\"Number of test examples:\", len(y_te))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lVZ37bEsdURD"},"source":["### Creation / load of stopwords list"]},{"cell_type":"code","metadata":{"id":"rxbc21sRdURD"},"source":["nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K8zCt_HidURE"},"source":["stop_words = stopwords.words('english')\n","\n","for i in range(1,21):\n","    stop_words.append(i*'x')\n","print(stop_words) # alphabetical sort"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jsd1L5-8dURF"},"source":["### Punctuation marks"]},{"cell_type":"code","metadata":{"id":"h5ahehHJdURG"},"source":["from string import punctuation\n","punctuation_marks = list(punctuation)\n","print(punctuation_marks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIbutUVVdURG"},"source":["stop_words_and_punctuation = sorted(list(set(stop_words + punctuation_marks)))\n","print(stop_words_and_punctuation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rUhj86eodURI"},"source":["case = 2\n","text = narratives_tr[case]\n","text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eFmYBZPKdURI"},"source":["## Bag-of-words (BOW) representation"]},{"cell_type":"code","metadata":{"id":"23hv_HB1dURJ"},"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","TF_vectorizer = CountVectorizer(max_features=1000, # max_df=0.5, min_df=20,\n","                                min_df=0.05,\n","                                stop_words=stop_words_and_punctuation)\n","\n","TF_vectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"unbFUFyfdURK"},"source":["TF_vectorizer.fit(narratives_tr)\n","tf_tr = TF_vectorizer.transform(narratives_tr)\n","TF_vocabulary = TF_vectorizer.get_feature_names_out()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sprvbq6kdURK"},"source":["print(len(TF_vocabulary))\n","print(TF_vocabulary)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z5dCkcU2dURL"},"source":["n=50\n","unique_labels = np.unique(labels)\n","for label in unique_labels:\n","    freqs = np.array(tf_tr[y_tr==label].sum(axis=0))[0]\n","    plt.figure(figsize=(15,5))\n","    inds = np.argsort(freqs)[::-1]\n","    plt.plot(freqs[inds[:n]])\n","    plt.xticks(range(n), np.array(TF_vocabulary)[inds[:n]], rotation=75)\n","    plt.title('word frequencies in complaints related to issue ' + '\"'+label+'\"', fontsize=16)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ESvjW9wdURM"},"source":["## TF-IDF"]},{"cell_type":"code","metadata":{"id":"cb7OFzkhdURM"},"source":["TFIDF_vectorizer = TfidfVectorizer(max_features=1000,\n","                                   min_df=0.05,\n","                                   stop_words=stop_words_and_punctuation,\n","                                   norm='l2')\n","print(TFIDF_vectorizer)\n","\n","X_tr  = np.array(TFIDF_vectorizer.fit_transform(narratives_tr).todense(),\n","                 dtype=np.float16)\n","X_te  = np.array(TFIDF_vectorizer.transform(narratives_te).todense(),\n","                 dtype=np.float16)\n","X_val = np.array(TFIDF_vectorizer.transform(narratives_val).todense(),\n","                 dtype=np.float16)\n","\n","TFIDF_vocabulary = TFIDF_vectorizer.get_feature_names_out()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3BhHrVXcdURN"},"source":["len(TFIDF_vocabulary)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Re5WXDQrdURO"},"source":["print(TFIDF_vocabulary)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"O3arnnvgdURO"},"source":["### Majority class (baseline)"]},{"cell_type":"code","metadata":{"id":"HaraPflrdURP"},"source":["from sklearn.dummy import DummyClassifier\n","\n","clf = DummyClassifier(strategy='most_frequent')\n","clf.fit(X_tr, y_tr)\n","print(\"score en training :\", clf.score(X_tr, y_tr).round(3))\n","print(\"score en test     :\", clf.score(X_te, y_te).round(3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_HxWOCGDdURQ"},"source":["## Autoencoder (los pesos del decoder son espejo de los del encoder)"]},{"cell_type":"code","metadata":{"id":"SkPUHD4LdURQ"},"source":["X_tr.shape, X_val.shape, X_te.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tCIs1xEldURQ"},"source":["np.unique(y_tr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VTy_w2wBdURR"},"source":["y_tr_int = 1*(y_tr==\"Credit card\")\n","y_te_int = 1*(y_te==\"Credit card\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekB8qjG3dURR"},"source":["# knn works better with normalized X_proy_tr, normalized X_proy_te"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U-4_Vo4rdURS"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras.callbacks import ModelCheckpoint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VLOa_3PLdURS"},"source":["def grafica_entrenamiento(tr_loss, val_loss):\n","    ax=plt.figure(figsize=(10,4)).gca()\n","    plt.plot(1+np.arange(len(tr_loss)), tr_loss)\n","    plt.plot(1+np.arange(len(val_loss)), val_loss)\n","    plt.title('loss del modelo', fontsize=18)\n","    plt.xlabel('época', fontsize=18)\n","    plt.ylabel('mse', fontsize=18)\n","    plt.legend(['entrenamiento', 'validación'], loc='upper left')\n","    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"32b8JpptdURT"},"source":["loss = \"mse\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t8FEEbH5dURT"},"source":["class DenseTranspose(keras.layers.Layer):\n","    def __init__(self, dense, activation=None, **kwargs):\n","        self.dense = dense\n","        self.activation = keras.activations.get(activation)\n","        super().__init__(**kwargs)\n","    def build(self, batch_input_shape):\n","        self.biases = self.add_weight(name=\"bias\",\n","                                      shape=[self.dense.input_shape[-1]],\n","                                      initializer=\"zeros\")\n","        super().build(batch_input_shape)\n","    def call(self, inputs):\n","        z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n","        return self.activation(z + self.biases)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KsjAWWrFdURU"},"source":["dense_1 = keras.layers.Dense(2, use_bias=False)\n","\n","encoder = keras.models.Sequential([dense_1])\n","decoder = keras.models.Sequential([DenseTranspose(dense_1)])\n","\n","ae = keras.models.Sequential([encoder, decoder])\n","\n","#ae.compile(loss=loss, optimizer=keras.optimizers.SGD(learning_rate=0.5))\n","#ae.compile(loss=loss, optimizer=keras.optimizers.SGD(learning_rate=1.5))\n","ae.compile(loss=loss, optimizer=\"adam\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qh4zjNikdURV"},"source":["modelpath=\"model_current_best.h5\"\n","checkpoint = ModelCheckpoint(modelpath, monitor='val_loss',\n","                             verbose=2,\n","                             save_best_only=True,\n","                             mode='min') # graba sólo los que mejoran en validación\n","\n","callbacks_list = [checkpoint]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mjYI-3DndURV"},"source":["epochs = 100\n","batch_size = 64\n","\n","acum_tr_loss = []\n","acum_val_loss = []\n","\n","best_weights = None\n","best_val_loss = 1e20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_92C3FhvdURW"},"source":["from copy import deepcopy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fdiIj1_gdURX"},"source":["for e in range(epochs):\n","    history = ae.fit(X_tr, X_tr,\n","                     batch_size=batch_size,\n","                     epochs=1,\n","                     #callbacks=callbacks_list,\n","                     verbose=0,\n","                     validation_data=(X_val, X_val))\n","\n","    acum_tr_loss  += history.history['loss']\n","    acum_val_loss += history.history['val_loss']\n","    \n","    if acum_val_loss[-1]<best_val_loss:\n","        best_val_loss = acum_val_loss[-1]\n","        best_weights = deepcopy(ae.get_weights())\n","    \n","    if (e+1)%50 == 0:\n","        grafica_entrenamiento(acum_tr_loss, acum_val_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"10ey48JRdURX"},"source":["ae.set_weights(best_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"age1LQkFdURX"},"source":["print(\"loss en training :\", ((np.array(X_tr) - ae.predict(X_tr))**2).mean())\n","print(\"loss en test     :\", ((np.array(X_te) - ae.predict(X_te))**2).mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"gWJYtJEQdURZ"},"source":["## Visualización del embedding aprendido"]},{"cell_type":"code","metadata":{"id":"3sS1uTkrdURZ"},"source":["doc_vecs_tr  = encoder.predict(X_tr)\n","doc_vecs_val = encoder.predict(X_val)\n","doc_vecs_te  = encoder.predict(X_te)\n","word_vecs = decoder.get_weights()[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XDhHDFd0dURa"},"source":["doc_vecs_tr.shape, doc_vecs_val.shape, doc_vecs_te.shape, word_vecs.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIRVp7XWdURa"},"source":["W1 = ae.layers[0].get_weights()[0]\n","W2 = ae.layers[1].get_weights()[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L2iAZadJdURb"},"source":["W1.shape, W2.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TJFixWBEdURc"},"source":["W1.T.dot(W2).round(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TtnhQqLNdURc"},"source":["# importing bokeh library for interactive dataviz\n","\n","import bokeh.plotting as bp\n","from bokeh.models import HoverTool, BoxSelectTool, LabelSet, ColumnDataSource, Range1d\n","from bokeh.plotting import figure, show, output_notebook\n","\n","output_notebook()\n","p = bp.figure(plot_width=700, plot_height=600, title=\"words in latent dimensions\",\n","              tools=\"pan,wheel_zoom,box_zoom,reset,hover,save\",\n","              x_axis_label='lat 0',\n","              y_axis_label='lat 1',\n","              #x_axis_type=None, y_axis_type=None,\n","              min_border=1)\n","p.title.text_font_size='16pt'\n","p.xaxis.axis_label_text_font_style='normal'\n","p.xaxis.axis_label_text_font_size='16pt'\n","p.yaxis.axis_label_text_font_style='normal'\n","p.yaxis.axis_label_text_font_size='16pt'\n","\n","p.xgrid.visible = False\n","p.ygrid.visible = False\n","\n","dictf = {'x':word_vecs[:,0],\n","         'y':word_vecs[:,1],\n","         'words':TFIDF_vocabulary}\n","aa = ColumnDataSource(dictf)\n","p.scatter(x='x', y='y', source=aa)\n","labels_p = LabelSet(x='x', y='y', text='words',\n","                    level='glyph',\n","                    x_offset=5, y_offset=5, source=aa, render_mode='canvas')\n","p.add_layout(labels_p)\n","\n","hover = p.select(dict(type=HoverTool))\n","hover.tooltips={\"word\": \"@words\"}\n","show(p)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CIXWMiCOdURd"},"source":["from sklearn.metrics.pairwise import pairwise_distances\n","\n","index2word = np.array(TFIDF_vocabulary)\n","word2index = {w:i for i,w in enumerate(index2word)}\n","\n","def most_similar_words(word,wordvecs,n=10):\n","    n_latent_dimensions = np.shape(wordvecs)[1]\n","    ind = word2index[word]\n","    aux = np.reshape(wordvecs[ind,:], (1,n_latent_dimensions))\n","    dists = pairwise_distances(aux, wordvecs).flatten()\n","    inds = dists.argsort().tolist()\n","    inds.remove(ind)\n","    return index2word[inds[:n]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5BlH_88RdURe"},"source":["len(TFIDF_vocabulary)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4MRMwLaYdURf"},"source":["most_similar_words('debt', word_vecs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EPOiUSlOdURf"},"source":["most_similar_words('card', word_vecs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yrn0FfC1dURh"},"source":["most_similar_words('loan', word_vecs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lKDkU5LLdURh"},"source":["most_similar_words('call', word_vecs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1vCQITb_dURh"},"source":["most_similar_words('bank', word_vecs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fQ5EgmzBdURi"},"source":["most_similar_words('home', word_vecs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lGiaBhbFdURj"},"source":["colors = ['blue', 'orange']\n","\n","plt.figure(figsize=(8,8))\n","for i,label in enumerate(unique_labels):\n","    plt.scatter(doc_vecs_tr[y_tr==label,0],\n","                doc_vecs_tr[y_tr==label,1],\n","                s = 1, alpha = 1, c = colors[i], label=label,\n","                )\n","plt.legend()\n","plt.xlabel('lat 0', fontsize=16)\n","plt.ylabel('lat 1', fontsize=16)\n","plt.title('docs in latent dimensions', fontsize=16);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_dLeBpdldURk"},"source":["def most_similar_docs(doc, wordvecs, docvecs, n=10):\n","    n_latent_dimensions = np.shape(wordvecs)[1]\n","    aux = np.reshape(doc, (1,n_latent_dimensions))\n","    dists = pairwise_distances(aux, docvecs).flatten()\n","    inds = dists.argsort().tolist()\n","    return inds[:n]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQjzFdM2dURk"},"source":["caso = 0\n","\n","doc_inds = most_similar_docs(doc_vecs_te[caso], word_vecs, doc_vecs_tr)\n","\n","start_bold = '\\033[1m'\n","end_bold   = '\\033[0m'\n","\n","print(start_bold + \"document {} in test:\".format(caso) + end_bold)\n","print(narratives_te[caso])\n","print(start_bold + \"Most similar documents in training:\" + end_bold)\n","for i,ind in enumerate(doc_inds):\n","    print(start_bold + \"* doc {} in training:\".format(i) + end_bold)\n","    print(narratives_tr[ind])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iniUgJoRdURl"},"source":["# defining the chart\n","output_notebook()\n","p = bp.figure(plot_width=700, plot_height=600, title=\"docs in latent dimensions\",\n","              tools=\"pan,wheel_zoom,box_zoom,reset,hover,save\",\n","              x_axis_label='lat 0',\n","              y_axis_label='lat 1',\n","              #x_axis_type=None, y_axis_type=None,\n","              min_border=1)\n","p.title.text_font_size = '16pt'\n","p.xaxis.axis_label_text_font_style='normal'\n","p.xaxis.axis_label_text_font_size='16pt'\n","p.yaxis.axis_label_text_font_style='normal'\n","p.yaxis.axis_label_text_font_size='16pt'\n","\n","p.xgrid.visible = False\n","p.ygrid.visible = False\n","\n","for i,label in enumerate(unique_labels):\n","    inds = np.where(y_tr==label)[0]\n","    dictf = {'x':doc_vecs_tr[inds,0],\n","             'y':doc_vecs_tr[inds,1],\n","             'text':np.array(narratives_tr)[y_tr==label],\n","             'ind':inds}\n","    aa = ColumnDataSource(dictf)\n","    p.scatter(x='x', y='y', source=aa,\n","              color=colors[i], legend=label)\n","\n","hover = p.select(dict(type=HoverTool))\n","hover.tooltips={\"text\": \"@text\", \"index in tr\": \"@ind\"}\n","show(p)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"xhfXu6pfdURm"},"source":["## Construction of classification models"]},{"cell_type":"markdown","metadata":{"id":"PwBI6XhHdURm"},"source":["### Naïve Bayes"]},{"cell_type":"code","metadata":{"id":"hInyZwQQdURn"},"source":["from sklearn.naive_bayes import GaussianNB\n","\n","clf = GaussianNB()\n","clf.fit(doc_vecs_tr, y_tr)\n","print(\"score en training :\", clf.score(doc_vecs_tr, y_tr).round(3))\n","print(\"score en test     :\", clf.score(doc_vecs_te, y_te).round(3))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dguzu5iTdURn"},"source":["### Support Vector Machines"]},{"cell_type":"code","metadata":{"id":"DEQNEW6JdURo"},"source":["from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ym4rpIB5dURp"},"source":["print(doc_vecs_tr.var(axis=0))\n","\n","clf = Pipeline(\n","    [(\"scaler\", StandardScaler()),\n","     (\"SVC\", SVC(kernel='rbf'))]\n",")\n","\n","clf.fit(doc_vecs_tr, y_tr)\n","print(\"score en training :\", clf.score(doc_vecs_tr, y_tr).round(3))\n","print(\"score en test     :\", clf.score(doc_vecs_te, y_te).round(3))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qlU2ypuqdURp"},"source":["### Logistic Regression"]},{"cell_type":"code","metadata":{"id":"Mg-ElVdydURp"},"source":["from sklearn.linear_model import LogisticRegression\n","\n","clf = Pipeline(\n","    [(\"scaler\", StandardScaler()),\n","     (\"LogReg\", LogisticRegression())]\n",")\n","\n","clf.fit(doc_vecs_tr, y_tr)\n","print(\"score en training :\", clf.score(doc_vecs_tr, y_tr).round(3))\n","print(\"score en test     :\", clf.score(doc_vecs_te, y_te).round(3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R4vrmfkhdURr"},"source":["from sklearn.metrics import classification_report, roc_curve, auc\n","\n","col_clase_positiva = 1\n","y_pred_proba = clf.predict_proba(doc_vecs_te)\n","y_pred = clf.predict(doc_vecs_te)\n","print('')\n","print(classification_report(y_te, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l6mPjNJidURr"},"source":["colors2 = ['r', 'g', 'm', 'c', 'y']\n","plt.figure(figsize=(7,5))\n","for i,l in enumerate(unique_labels):\n","    fpr, tpr, thresholds = roc_curve(y_te, y_pred_proba[:,i], pos_label=unique_labels[i])\n","    plt.plot(fpr, tpr, colors2[i]+'-.', label=l+' (%2.2f)' % auc(fpr, tpr))\n","plt.plot(fpr, fpr, 'b-', label = 'Random Guess')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.legend();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"c1mfg6r2dURr"},"source":["# Clustering"]},{"cell_type":"code","metadata":{"id":"YDMUpEnSdURs"},"source":["scaler = StandardScaler()\n","X_km   = scaler.fit_transform(doc_vecs_tr)\n","\n","X_km = doc_vecs_tr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"KTFI5I-FdURt"},"source":["## k-means"]},{"cell_type":"code","metadata":{"id":"f46D25eJdURt"},"source":["from sklearn.cluster import KMeans\n","from sklearn.metrics import calinski_harabasz_score as qmetric\n","\n","Nrepetitions = 10\n","\n","qualities = []\n","inertias = []\n","models = []\n","kini = 1\n","kfin = 10\n","for k in range(kini,kfin+1):\n","    print(\"Evaluando k=%d\" % k)\n","    km = KMeans(n_clusters=k,\n","                init='k-means++', n_init=Nrepetitions,\n","                max_iter=500, random_state=2)    \n","    km.fit(X_km)\n","    models.append(km)\n","    inertias.append(km.inertia_)\n","    if k >1:\n","        qualities.append(qmetric(X_km, km.labels_))\n","    else:\n","        qualities.append(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OwKIUlfGdURu"},"source":["fig = plt.figure(figsize=(14,3))\n","\n","ax = plt.subplot(1,2,1)\n","plt.plot(range(kini,kfin+1), inertias, marker='o')\n","plt.xlabel('number of clusters')\n","plt.title('clustering inertia')\n","\n","ax = plt.subplot(1,2,2)\n","plt.plot(range(kini,kfin+1), qualities, marker='o')\n","plt.xlabel('number of clusters')\n","plt.title('clustering quality')\n","plt.show()\n","\n","best = pd.Series(qualities).idxmax() # get index for the best model\n","km = models[best]\n","n_clusters = km.get_params()['n_clusters']\n","clusters = km.labels_\n","n_clusters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d3Kh1xhAdURu"},"source":["def clustering_reporting(cluster_labels, clases):\n","    unique_clases = np.unique(clases)\n","    \n","    for i,c in enumerate(np.unique(cluster_labels)):\n","        inds = np.where(cluster_labels == c)[0]\n","        print(\"cluster %d (%.2f%%):\" % (i,100*len(inds)/len(clases)))\n","        for c in unique_clases:\n","            print('   '+c+\": %.2f%%\" % (100*(clases[inds].tolist().count(c))/len(inds)))\n","        print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"VnbmbsR0dURv"},"source":["clustering_reporting(km.labels_, y_tr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gaaV7Tv4dURw"},"source":[],"execution_count":null,"outputs":[]}]}